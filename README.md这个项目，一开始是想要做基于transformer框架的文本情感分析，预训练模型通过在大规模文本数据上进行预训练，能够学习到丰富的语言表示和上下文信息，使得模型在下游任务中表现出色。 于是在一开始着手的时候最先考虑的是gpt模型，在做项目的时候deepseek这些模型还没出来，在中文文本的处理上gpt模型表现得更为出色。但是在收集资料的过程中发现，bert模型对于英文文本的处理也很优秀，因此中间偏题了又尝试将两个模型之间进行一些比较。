这个项目，一开始是想要做基于transformer框架的文本情感分析，预训练模型通过在大规模文本数据上进行预训练，能够学习到丰富的语言表示和上下文信息，使得模型在下游任务中表现出色。
于是在一开始着手的时候最先考虑的是gpt模型，在做项目的时候deepseek这些模型还没出来，在中文文本的处理上gpt模型表现得更为出色。但是在收集资料的过程中发现，bert模型对于英文文本的处理也很优秀，因此中间偏题了又尝试将两个模型之间进行一些比较。
但是由于这个项目是第一次尝试，过程中的数据源选择以及模型的调整等都比较不成熟，甚至对于github的使用也很不熟练，但也确实是很不一样的一次尝试
