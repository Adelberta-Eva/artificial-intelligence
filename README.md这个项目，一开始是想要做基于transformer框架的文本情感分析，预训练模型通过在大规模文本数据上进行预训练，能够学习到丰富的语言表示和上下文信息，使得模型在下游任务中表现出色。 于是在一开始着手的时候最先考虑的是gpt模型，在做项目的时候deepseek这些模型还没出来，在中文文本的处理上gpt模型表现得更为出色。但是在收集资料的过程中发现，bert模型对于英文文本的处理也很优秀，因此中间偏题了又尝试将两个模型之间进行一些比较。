本项目最是想要基于 Transformer 架构进行文本情感分析，利用预训练语言模型在大规模语料上的预训练能力，来捕捉更深层次的语言表达与上下文语义信息，从而提升下游任务的情感分类效果。   

在项目一开始，DeepSeek 等较新模型尚未问世。在收集资料之后，考虑到中文文本处理的性能表现，当时优先选用了 GPT 系列模型作为初步实验对象。   

然而，在后续文献调研中，我发现 BERT 在英文语料的情感分析任务中展现出较高的鲁棒性和泛化能力，于是拓展了项目方向，尝试对 GPT 与 BERT 在中英文语境下的表现进行对比分析。    

在模型部署的时候，最初计划通过调用 Hugging Face 提供的在线接口直接使用模型，但由于网络不稳定，经常在训练到一半时的时候断开连接，所以最终选择将模型权重下载至本地进行实验。  

在数据集选择阶段，由于使用数据集的数据量规模较小，且标签划分存在一定偏差，最后的实验结果是模型在训练过程中的收敛效果不佳，最终在验证集和测试集上的表现也不尽理想。   

这导致后面在其他的项目尝试的时候，很重视数据集的选择。
